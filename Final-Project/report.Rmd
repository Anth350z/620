---
title: "Network & Text Analysis of Yelp Reviews"
author: "Juliann McEachern, Mia Siracusa, Anthony Munoz, Katie Evers"
date: "July 17, 2019"
output: 
  html_document:
    theme: paper
    highlight: pygments
    toc: true
    toc_float: true
    toc_depth: 2
    df_print: paged
    code_folding: show
---


# Final Project 

For the final project, we will be examining the relationship between yelp reviews and businesses in the metropolitan area of Phoenix, Arizona. The data was obtained from Kaggle's 2013 Yelp Review Challenge and was subsetted to include only businesses within the food and beverage industries.

We have a two-part goal in this assignment:

1.  Identify the links between key Yelp users and businesses within the Phoenix, Arizona community.  
2.  Evaluate the relationship between the text contents of reviews and the rating a business received. 

```{r dependencies, echo=F, warning=F, message=F, comment=F}
##network packages
library(igraph)

## text processing packages
library(plyr);library(dplyr); library(tidyverse); library(stringr); library(tidytext); library(textdata)

##formatting packages
library(knitr); library(kableExtra); library(default)

## knit sizing
options(max.print="100"); opts_knit$set(width=75) 

## augment chunk output
opts_chunk$set(echo=T,cache=F, tidy=T,comment=F,message=T,warning=T) #change message/warning to F upon completion


## set table style for consistency
default(kable) <- list(format="html")
default(kable_styling)  <- list(bootstrap_options = "hover",full_width=T, font_size=10)
default(scroll_box) <- list(width = "100%")
```

## Data Aquisition & Tidying {.tabset .tabset-fade .tabset-pills}

Data was acquired from Kaggle as a JSON file for a project conducted in [Data 612](http://rpubs.com/jemceach/D212-Final-Project). Our network uses the subsetted data from that project. This subset is stored as a csv file in our data folder and was read into this report for further review.  

We added additional transformations to meet our project goals and separated the data into seperate dataframes for network building and text processing. 

```{r aquisition-tidying, echo=F}
# load data
yelp<- read.csv("data/yelp.csv"); yelp <- select(yelp,-X, -business_id, -user_id) %>% rename(businessID = itemID)

# transform data for network analysis 
yelp_network <- yelp %>% 
  select(userID, businessID, name, stars, latitude, longitude) %>% 
  group_by(businessID) %>% 
  add_count() %>%
  rename(size = n)%>%
  ungroup() %>%
  group_by(userID) %>% 
  add_count() %>%
  rename(weight = n)%>%
  ungroup() 

# transform data for text processing 
yelp_reviews <- yelp %>% 
  transform(reviewID=match(review_id, unique(review_id))) %>% 
  select(-review_id, -longitude, -latitude) %>%
  select(userID, businessID, reviewID, name, city, state, votes.funny, votes.useful, votes.cool, 
         stars, text, date)
```

### Network Data 
```{r view-network-data, echo=F}
yelp_network
```


### Text Data 

```{r view-text-data, echo=F}
yelp_reviews
```


# Network Analysis

## 2-Mode Network  {.tabset .tabset-fade .tabset-pills}

We set up our initial 2-mode network by connecting our businesses and users using a weighted incidence matrix. We plotted our graph using the `plot.igraph` function and verified our network was created properly. 

### Build Network

```{r build-network}
# define edges; spread data from long to wide; convert to matrix
edges <- yelp_network %>% 
  select(businessID, userID, weight) %>% 
  spread(businessID, weight, fill = 0) %>% 
  column_to_rownames('userID') %>%
  as.matrix()

# define nodesets 
business_nodes <- yelp_network %>% 
  select(businessID, name, size) %>% 
  mutate(type = 'business', name = as.character(name)) %>% 
  distinct()

user_nodes <- yelp_network %>% 
  select(userID) %>% 
  mutate(name=paste0("U",userID),sizes = NA, type = 'user') %>% 
  distinct()

# bind rows
nodes <- bind_rows(business_nodes,user_nodes)

# initiate graph from matrix
g <- graph_from_incidence_matrix(edges, weighted=T)

# Define vertex color/shape
V(g)$shape <- ifelse(V(g)$type, "circle", "square")
V(g)$color <- ifelse(V(g)$type, "red", "white")
```

### Network Graph

```{r plot-graph, echo=F}
# Plot network
plot.igraph(g, 
            layout=layout.bipartite, 
            vertex.frame.color="black",
            vertex.label=NA)
```

### Verification

```{r verify-network, echo=F}
# verify vertices (F = User; T = Business)
node_count <-table(V(g)$type==T)

# Verify connections
rbind(Business.Nodes = toString(node_count[2]), 
      User.Nodes = toString(node_count[1]),
      Is.Weighted = toString(is.weighted(g)), 
      Is.Bipartite = toString(is.bipartite(g))) %>%
  kable(caption="Verify Node Counts and Connectivity") %>%
  kable_styling()
```

## Edge-Trimming  {.tabset .tabset-fade .tabset-pills}

To better understand our network, we applied the island method to see our most influential user and businesses within our dataset.

```{r}
#In order to see our network better, we are going to sparsify it by only keeping only the most important ties and discarding the rest

#Modify data frame
edgesDf <- yelp_network %>% 
  select(businessID, userID, weight)

#Convert weight to numeric
weight <- as.numeric(unlist(edgesDf$weight))

head(edgesDf)

#Examine frquency of weight
hist(weight)
#Calculate mean and standard deviation
mean(weight)
sd(weight)

#Keep edges that have weight higher than the mean
cut.off <- mean(weight)
net.sp <- delete_edges(g, E(g)[weight<cut.off])
plot(net.sp)


#It is still difficult to see the network so we will try eliminating weak vertices

#Eliminate vertices with degree 0
net.sp <- delete.vertices(g, 
            V(g)[ degree(g) == 0] )
plot(net.sp)

#Calculate mean degrees of network
mean(degree(g))

#Eliminate vertices with degreea less than rounded mean
net.sp <- delete.vertices(g, 
            V(g)[ degree(g) < 6] )
plot(net.sp)


#We can see the most influential user and businesses from the visualization. The user and business names are identified in the tibbles below.

#User IDs and names tibble
rbind(user_nodes[2308,], user_nodes[3667,], user_nodes[1342,]) 

#Business IDs and names tibble
rbind(business_nodes[2244,], business_nodes[1239,], business_nodes[4102,], business_nodes[2704,])
```

# Text Analysis

We would like to asses each word in the reviews as either positive or negative and find the difference between the number of positive and negative words. This will be the "score" of the review. 

## Sentiment Function

First, we will use the Bing sentiment lexicon from the tidytext package. The Bing lexicon classifies certain words as either positive or negative. 

```{r}
# positive / negative sentiment function
m <- get_sentiments("bing")

pos.words <- vector()
neg.words <- vector()

for (i in 1:nrow(m)) {
    if (m$sentiment[i] == "positive") {
        pos.words[i] <- m$word[i]
    }
}


for (i in 1:nrow(m)) {
    if (m$sentiment[i] == "negative") {
        neg.words[i] <- m$word[i]
    }
}


pos.words[5]
neg.words[5]
```

Then, we created a function that splits the string of text form the reviews and analyzes each word, and then calculates the score. **Reference source.**

```{r}
v <- as.character(as.vector(yelp_reviews$text))

score.sentiment = function(v, pos.words, neg.words, .progress = "none") {
    require(plyr)
    require(stringr)
    
    # we got a vector of sentences. plyr will handle a list or a vector as an
    # "l" for us we want a simple array ("a") of scores back, so we use "l" +
    # "a" + "ply" = "laply":
    
    scores = laply(v, function(sentence, pos.words, neg.words) {
        
        # clean up sentences with R's regex-driven global substitute, gsub():
        sentence = gsub("[[:punct:]]", "", sentence)
        sentence = gsub("[[:cntrl:]]", "", sentence)
        sentence = gsub("\\d+", "", sentence)
        # and convert to lower case:
        sentence = tolower(sentence)
        
        # split into words. str_split is in the stringr package
        word.list = str_split(sentence, "\\s+")
        # sometimes a list() is one level of hierarchy too much
        words = unlist(word.list)
        
        # compare our words to the dictionaries of positive & negative terms
        pos.matches = match(words, pos.words)
        neg.matches = match(words, neg.words)
        
        # match() returns the position of the matched term or NA we just want a
        # TRUE/FALSE:
        pos.matches = !is.na(pos.matches)
        neg.matches = !is.na(neg.matches)
        
        # and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
        score = sum(pos.matches) - sum(neg.matches)
        
        return(score)
    }, pos.words, neg.words, .progress = .progress)
    
    scores.df = data.frame(score = scores, text = v)
    return(scores.df)
}
```

## Visualize Relationships {.tabset .tabset-fade .tabset-pills}

We added the scores for each review to the original dataframe. Once this step is finished, it will make analysis much easier. Below, I've compared some of the variables to explore the relationships between score of reviews and the stars associated with the reviews.

```{r}
t <- score.sentiment(v, pos.words, neg.words)
full <- inner_join(t, yelp_reviews, by = "text")
```


### Stars Score

```{r}
plot(full$stars, full$score)
```

### Funny Score
```{r}
plot(full$votes.funny, full$score)
```

### Useful Score
```{r}
plot(full$votes.useful, full$score)
```

### Cool Score
```{r}
plot(full$votes.cool, full$score)
```

### Analysis 

The most interesting plot to examine further would be the stars vs score. It appears that as stars increase, so does the score. We will test if there is actually a relationship.

## Correlation
```{r}
cor(full$stars, full$score, method = c("pearson", "kendall", "spearman"))
```

The p-value is .04418, so at a 95% confidence interval, we can say that there is positive relationship between stars and the amount of positive words in the reviews of restaurants.

```{r}
cor.test(full$stars, full$score, method = c("pearson", "kendall", "spearman"))
```


# Conclusion

Final thoughts. 

----------

# References

Inspiration for this project was derrived from the following sources: 

1.  Data Source: https://www.kaggle.com/c/yelp-recsys-2013/data
2.  Related Project: http://rpubs.com/jemceach/D612-Final-Project
3.  R Network Reference: https://kateto.net/network-visualization


